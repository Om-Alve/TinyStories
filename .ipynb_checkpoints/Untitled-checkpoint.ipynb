{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e5fb39-4c8d-43de-a430-565b1f99794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model,GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2d4ba0f-db05-4012-973d-058cad569100",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=50257,  # Vocabulary size of the GPT-2 model\n",
    "    n_embd=240,  # Hidden size of the transformer embeddings\n",
    "    n_layer=10,  # Number of transformer layers\n",
    "    n_head=10,  # Number of attention heads\n",
    "    n_positions=1024,  # Maximum sequence length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4143dcd1-690e-4b66-b250-c36de797506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a937e0a-e37f-4fac-9847-443f325c6e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 240)\n",
       "  (wpe): Embedding(1024, 240)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-9): 10 x GPT2Block(\n",
       "      (ln_1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e03c6b7-a06d-4dea-9800-9e923c12ed19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.25112"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.parameters()]) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c2758945-b6c4-4660-93b3-473a20d6615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchmetrics\n",
    "import random\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class GPT2Config:\n",
    "    vocab_size: int\n",
    "    embed_dim: int\n",
    "    block_size: int\n",
    "    num_heads: int\n",
    "    num_layers: int\n",
    "    dropout: float\n",
    "    lr: float\n",
    "    t_max: int\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        hidden_size = int(config.embed_dim * (4 * (2 / 3)))\n",
    "        self.linear_w = nn.Linear(config.embed_dim, hidden_size, bias=False)\n",
    "        self.linear_v = nn.Linear(config.embed_dim, hidden_size, bias=False)\n",
    "        self.linear_w2 = nn.Linear(hidden_size, config.embed_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.linear_w.weight)\n",
    "        nn.init.xavier_uniform_(self.linear_v.weight)\n",
    "        nn.init.xavier_uniform_(self.linear_w2.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.dropout(self.linear_w2(F.silu(self.linear_w(x)) * self.linear_v(x)))\n",
    "        return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        self.head_dim = config.embed_dim // config.num_heads\n",
    "        self.qkv = nn.Linear(config.embed_dim, 3 * self.head_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.qkv.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, training: bool) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.head_dim).permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv[:, 0], qkv[:, 1], qkv[:, 2]\n",
    "        out = torch.nn.functional.scaled_dot_product_attention(\n",
    "            q, k, v, attn_mask=None, dropout_p=self.dropout.p if training else 0.0, is_causal=True\n",
    "        )\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(config) for _ in range(config.num_heads)])\n",
    "        self.proj = nn.Linear(config.embed_dim, config.embed_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, training: bool) -> torch.Tensor:\n",
    "        out = torch.cat([head(x, training) for head in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return self.dropout(out)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.ffwd = SwiGLU(config)\n",
    "        self.ln1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(config.embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, training: bool) -> torch.Tensor:\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.attention(x, training)\n",
    "        x = self.ln2(x)\n",
    "        return x + self.ffwd(x)\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(config.block_size, config.embed_dim)\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.num_layers)])\n",
    "        self.ln_f = nn.LayerNorm(config.embed_dim)\n",
    "        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size)\n",
    "        # Weight Tying\n",
    "        self.token_embeddings.weight = self.lm_head.weight\n",
    "\n",
    "    def forward(self, x: torch.Tensor, training: bool = False) -> torch.Tensor:\n",
    "        batch_size, seq_len = x.shape\n",
    "        token_emb = self.token_embeddings(x)\n",
    "        pos_emb = self.position_embeddings(torch.arange(seq_len, device=x.device))\n",
    "        x = token_emb + pos_emb\n",
    "        for block in self.blocks:\n",
    "            x = block(x, training)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, tokenizer, max_tokens: int, text: str = \"\", temperature: float = 0.0) -> str:\n",
    "        self.eval()\n",
    "        if text == \"\":\n",
    "            text = tokenizer.bos_token\n",
    "        input_ids = tokenizer(text=text, return_tensors='pt')['input_ids']\n",
    "        for _ in range(max_tokens):\n",
    "            input_ids = input_ids[:, -self.config.block_size:]\n",
    "            logits = self(input_ids)[:, -1, :]\n",
    "            if temperature == 0.0:\n",
    "                _, next_token = torch.topk(logits, k=1, dim=-1)\n",
    "            else:\n",
    "                logits = logits / temperature\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            input_ids = torch.cat((input_ids, next_token), dim=1)\n",
    "        self.train()\n",
    "        return tokenizer.decode(token_ids=input_ids[0])\n",
    "\n",
    "class GPT2(L.LightningModule):\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "        self.model = TransformerDecoder(config)\n",
    "        self.lr = config.lr\n",
    "        self.t_max = config.t_max\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, tokenizer, max_tokens: int, text: str = \"\", temperature: float = 0.0) -> str:\n",
    "        return self.model.generate(tokenizer=tokenizer, text=text, max_tokens=max_tokens, temperature=temperature)\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        input_ids, targets = batch['input_ids'], batch['input_ids'][..., 1:].contiguous()\n",
    "        logits = self(input_ids)[:, :-1, :].contiguous()\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1), ignore_index=50256)\n",
    "        self.log('training_loss', loss, prog_bar=True, on_step=True, on_epoch=False, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> None:\n",
    "        logits = self(batch['input_ids'])\n",
    "        targets = batch['input_ids'][..., 1:].contiguous()\n",
    "        logits = logits[:, :-1, :].contiguous()\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1), ignore_index=50256)\n",
    "        self.log('validation_loss', loss, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self) -> Dict[str, Any]:\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=opt, T_max=self.t_max)\n",
    "        return {\n",
    "            'optimizer': opt,\n",
    "            'lr_scheduler': {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"training_loss\",\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "            }\n",
    "        }\n",
    "\n",
    "class GenerateCallback(L.pytorch.callbacks.Callback):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def on_epoch_end(self, trainer: L.Trainer, pl_module: GPT2) -> None:\n",
    "        generated_text = pl_module.generate(tokenizer=self.tokenizer, max_tokens=256, temperature=1.0)\n",
    "        print(\"Generated text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8ea329fc-44f5-458d-b2ca-cd29334a3202",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GPT2Config.__init__() got an unexpected keyword argument 'n_embed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[180], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2Config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mn_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50257\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mn_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mt_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m TransformerDecoder(block_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,n_embed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50257\u001b[39m,n_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,n_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: GPT2Config.__init__() got an unexpected keyword argument 'n_embed'"
     ]
    }
   ],
   "source": [
    "config = GPT2Config(block_size=1024,n_embed=100,vocab_size=50257,n_heads=1,n_layers=1,dropout=0.0,t_max=0,lr=0.0)\n",
    "model = TransformerDecoder(block_size=1024,n_embed=100,vocab_size=50257,n_heads=1,n_layers=1,dropout=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "99e8dffd-2a13-4dd4-a04f-0a80d2a8ad24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2459,  0.4977,  1.0184,  ...,  0.0103,  0.0598, -0.4062]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.ones((1,1),dtype=torch.long),training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c8a4ef-514d-4a77-bcdb-ada7d5fce669",
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acehacks",
   "language": "python",
   "name": "acehacks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
