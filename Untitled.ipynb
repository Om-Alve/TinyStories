{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e5fb39-4c8d-43de-a430-565b1f99794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model,GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2d4ba0f-db05-4012-973d-058cad569100",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=50257,  # Vocabulary size of the GPT-2 model\n",
    "    n_embd=240,  # Hidden size of the transformer embeddings\n",
    "    n_layer=10,  # Number of transformer layers\n",
    "    n_head=10,  # Number of attention heads\n",
    "    n_positions=1024,  # Maximum sequence length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4143dcd1-690e-4b66-b250-c36de797506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a937e0a-e37f-4fac-9847-443f325c6e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 240)\n",
       "  (wpe): Embedding(1024, 240)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-9): 10 x GPT2Block(\n",
       "      (ln_1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e03c6b7-a06d-4dea-9800-9e923c12ed19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.25112"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.parameters()]) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c2758945-b6c4-4660-93b3-473a20d6615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchmetrics\n",
    "import random\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPT2Config:\n",
    "    vocab_size :int\n",
    "    n_embed :int\n",
    "    block_size :int\n",
    "    n_heads :int\n",
    "    n_layers :int\n",
    "    dropout :float\n",
    "    lr :float\n",
    "    t_max :int    \n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, n_embed: int):\n",
    "        super().__init__()\n",
    "        hidden_size = int(n_embed * (4 * (2/3)))  # Corrected calculation\n",
    "        self.w = nn.Linear(n_embed, hidden_size, bias=False)\n",
    "        self.v = nn.Linear(n_embed, hidden_size, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_size, n_embed, bias=False)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        nn.init.xavier_uniform_(self.w.weight)\n",
    "        nn.init.xavier_uniform_(self.v.weight)\n",
    "        nn.init.xavier_uniform_(self.w2.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.dropout(self.w2(F.silu(self.w(x)) * self.v(x)))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size: int, n_embed: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(n_embed, 3 * head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.xavier_uniform_(self.qkv.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, training: bool) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, T, 3, -1).permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv[:, 0], qkv[:, 1], qkv[:, 2]\n",
    "        out = torch.nn.functional.scaled_dot_product_attention(\n",
    "            q, k, v, attn_mask=None, dropout_p=self.dropout.p if training else 0.0, is_causal=True\n",
    "        )\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, head_size: int, n_embed: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size,n_embed, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed,bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, training: bool) -> torch.Tensor:\n",
    "        out = torch.cat([head(X, training) for head in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return self.dropout(out)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.sa_heads = MultiHeadAttention(n_heads, n_embed // n_heads, n_embed, dropout)\n",
    "        self.ffwd = SwiGLU(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, training: bool) -> torch.Tensor:\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.sa_heads(x, training)\n",
    "        x = self.ln2(x)\n",
    "        return x + self.ffwd(x)\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, config :GPT2Config):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(config.vocab_size, config.n_embed)\n",
    "        self.positon_embedding = nn.Embedding(config.block_size, config.n_embed)\n",
    "        self.blocks = nn.ModuleList([Block(config.n_embed, config.n_heads, config.dropout) for _ in range(config.n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(config.n_embed)\n",
    "        self.lm_head = nn.Linear(config.n_embed, config.vocab_size)\n",
    "        # Weight Tying\n",
    "        self.embedding_table.weight = self.lm_head.weight\n",
    "\n",
    "    def forward(self, x: torch.Tensor, training: bool = False) -> torch.Tensor:\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.embedding_table(x)\n",
    "        pos_emb = self.positon_embedding(torch.arange(T, device=x.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.blocks:\n",
    "            x = block(x, training)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits        \n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def generate(self, tokenizer, max_tokens: int, text: str = \"\", temperature: float = 0.0) -> str:\n",
    "        self.eval()\n",
    "        if text == \"\":\n",
    "            text = tokenizer.bos_token\n",
    "        idxs = tokenizer(text=text, return_tensors='pt')['input_ids']\n",
    "        for _ in range(max_tokens):\n",
    "            idxs = idxs[:, -512:]\n",
    "            logits = self(idxs)[:, -1, :]\n",
    "            if temperature == 0.0:\n",
    "                _, next_token = torch.topk(logits, k=1, dim=-1)\n",
    "            else:\n",
    "                logits = logits / temperature\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            idxs = torch.cat((idxs, next_token), dim=1)\n",
    "        self.train()\n",
    "        return tokenizer.decode(token_ids=idxs[0])\n",
    "            \n",
    "class GPT2(L.LightningModule):\n",
    "    def __init__(self, config :GPT2Config):\n",
    "        super().__init__()\n",
    "        self.model = TransformerDecoder(config)\n",
    "        self.lr = config.lr\n",
    "        self.t_max = config.t_max\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def generate(self, tokenizer, max_tokens: int, text: str = \"\", temperature: float = 0.0) -> str:\n",
    "        return self.model.generate(tokenizer=tokenizer, text=text, max_tokens=max_tokens, temperature=temperature)\n",
    "    \n",
    "    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        logits = self(batch['input_ids'])\n",
    "        targets = batch['input_ids'][..., 1:].contiguous()\n",
    "        logits = logits[:, :-1, :].contiguous()\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1), ignore_index=50256)\n",
    "        self.log('training_loss', loss, prog_bar=True, on_step=True, on_epoch=False, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> None:\n",
    "        logits = self(batch['input_ids'])\n",
    "        targets = batch['input_ids'][..., 1:].contiguous()\n",
    "        logits = logits[:, :-1, :].contiguous()\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1), ignore_index=50256)\n",
    "        self.log('validation_loss', loss, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self) -> Dict[str, Any]:\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=opt, T_max=self.t_max)\n",
    "        return {\n",
    "            'optimizer': opt,\n",
    "            'lr_scheduler': {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"training_loss\",\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "            }\n",
    "        }\n",
    "\n",
    "class GenerateCallback(L.pytorch.callbacks.Callback):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def on_epoch_end(self, trainer: L.Trainer, pl_module: GPT2) -> None:\n",
    "        generated_text = pl_module.generate(tokenizer=self.tokenizer, max_tokens=256, temperature=1.0)\n",
    "        print(\"Generated text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8ea329fc-44f5-458d-b2ca-cd29334a3202",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(block_size=1024,n_embed=100,vocab_size=50257,n_heads=1,n_layers=1,dropout=0.0,t_max=0,lr=0.0)\n",
    "model = TransformerDecoder(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "99e8dffd-2a13-4dd4-a04f-0a80d2a8ad24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1345,  0.5670, -0.6863,  ..., -0.5266, -0.0247, -0.1971]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()(torch.ones((1,1),dtype=torch.long),training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c8a4ef-514d-4a77-bcdb-ada7d5fce669",
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acehacks",
   "language": "python",
   "name": "acehacks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
